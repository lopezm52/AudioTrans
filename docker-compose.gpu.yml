version: '3.8'

services:
  audiotrans-gpu:
    build:
      context: .
      dockerfile: Dockerfile.gpu
    container_name: audiotrans-gpu
    ports:
      - "8001:8001"
    environment:
      # API Configuration
      - API_KEY=${API_KEY:-audio-trans-secret-key-2024}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      
      # Whisper Configuration
      - WHISPER_MODEL=${WHISPER_MODEL:-medium}
      - MAX_FILE_SIZE=${MAX_FILE_SIZE:-200MB}
      
      # GPU Configuration
      - CUDA_VISIBLE_DEVICES=0
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
      
      # Performance Settings
      - OMP_NUM_THREADS=4
      - PYTHONUNBUFFERED=1
      
    volumes:
      # Mount for model persistence
      - whisper_models:/home/appuser/.cache/whisper
      # Optional: mount local code for development
      # - ./app.py:/app/app.py
    
    # GPU Runtime Configuration
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
        limits:
          memory: 8G
          cpus: '4.0'
    
    # Health check optimized for GPU startup time
    healthcheck:
      test: ["CMD", "python3.11", "-c", "import requests; requests.get('http://localhost:8001/health', timeout=30)"]
      interval: 60s
      timeout: 35s
      start_period: 180s  # Longer startup time for GPU model loading
      retries: 3
    
    # Restart policy
    restart: unless-stopped
    
    # Logging configuration
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "3"

volumes:
  whisper_models:
    driver: local 